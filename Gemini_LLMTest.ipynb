{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b318486",
   "metadata": {},
   "source": [
    "## Setup Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d65a1913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google import genai\n",
    "from google.genai.errors import APIError \n",
    "\n",
    "# Paste your API Key here\n",
    "API_KEY = \"AIzaSyCbbuvvBC8LQ-dN26-MwrTQASqg6pQyMBo\" \n",
    "\n",
    "# Initialize Gemini client\n",
    "try:\n",
    "    client = genai.Client(api_key=API_KEY)\n",
    "    print(\"Gemini client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR: Could not initialize Gemini client. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Pick a model\n",
    "MODEL_NAME = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9b6860b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# You can run this line to see if your API key is even working\\nclient = genai.Client(API_KEY)\\nresponse = client.models.generate_content(\\n    model=\"gemini-2.5-flash\", contents=\"What is the capital of France? Respond with only the city name.\")\\nprint(response.text)\\n\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# You can run this line to see if your API key is even working\n",
    "client = genai.Client(API_KEY)\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"What is the capital of France? Respond with only the city name.\")\n",
    "print(response.text)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3134c8",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fd1bda",
   "metadata": {},
   "source": [
    "# Data Analysis: **High Stress Moments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "313c3f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in High Stress Moments file as dataframe\n",
    "import pandas as pd\n",
    "highstress_df = pd.read_csv('/Users/veronica.upadhyay/Downloads/Gemini_Usages/High Stress Moments - PCO Analysis - DeID Copy of Agged Responses.csv')\n",
    "highstress_df = highstress_df[['Client', 'consumer_id', 'agg_message_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb8c77a",
   "metadata": {},
   "source": [
    "#### How often do Well Guides followup about the High Stress Moments PCO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "718aef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a specialized text analysis system designed to review chat transcripts between a \"CONSUMER\" and an \"AGENT.‚Äù The chat messages are formatted as follows: ROLE (TIMESTAMP): MESSAGE. \n",
    "For example: CONSUMER (2024-10-21 13:44:00+00): talking about things. \n",
    "You will examine a message exchange between a CONSUMER and AGENT to determine if the AGENT at any point sent a message that seems to be following up about stress management tips. Respond **only** with \"Yes\" if there is a followup message about stress management tips, and **only** with \"No\" if there is not.\n",
    "\"\"\"\n",
    "USER_QUESTION = \"Analyze the following chat messages and determine if the AGENT sent a followup message about stress management tips:\"\n",
    "\n",
    "# Establish the new column \n",
    "highstress_df['wg_followup'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90b3bd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 7: Result stored as 'NO'\n",
      "Index 8: Result stored as 'NO'\n",
      "Index 9: Result stored as 'YES'\n",
      "Index 10: Result stored as 'NO'\n",
      "Index 11: Result stored as 'NO'\n",
      "                                    agg_message_text wg_followup\n",
      "1  CONSUMER (2025-05-08 16:21:00+00): stuck\\nAGEN...            \n",
      "2  CONSUMER (2025-05-08 17:08:00+00): Stuck\\nAGEN...            \n",
      "3  CONSUMER (2025-05-08 17:12:00+00): Stuck\\nAGEN...            \n",
      "4  CONSUMER (2025-05-08 18:03:00+00): stuck\\nAGEN...            \n",
      "5  CONSUMER (2025-05-08 20:21:00+00): Stuck\\nAGEN...            \n"
     ]
    }
   ],
   "source": [
    "# Iterate prompt through a limit number of rows\n",
    "for index, row in highstress_df.iloc[7:12].iterrows(): \n",
    "    chat_transcript = row['agg_message_text']\n",
    "    \n",
    "    # 1. Simplify the user content (only include the question and data)\n",
    "    user_content = f\"{USER_QUESTION}\\nChat Messages:\\n{chat_transcript}\"\n",
    "    \n",
    "    try:\n",
    "        # Call the Gemini API using the simpler single-content structure for this task\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_NAME,\n",
    "            # 2. Pass the simplified user content\n",
    "            contents=user_content, \n",
    "            config=dict(\n",
    "                temperature=0.0,\n",
    "                #max_output_tokens=1000,\n",
    "                # 3. Add the System Instruction here!\n",
    "                system_instruction=SYSTEM_PROMPT\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # --- ROBUST RESPONSE HANDLING ---\n",
    "        # 1. Check if the text is present\n",
    "        if response.text is not None and response.text.strip():\n",
    "            answer = response.text.strip().upper()\n",
    "        \n",
    "        # 2. Check for blocking or empty response\n",
    "        else:\n",
    "            reason = 'NO_CANDIDATES'\n",
    "            \n",
    "            # Use getattr to safely access nested attributes, checking for a block reason\n",
    "            block_reason = getattr(getattr(response, 'prompt_feedback', None), 'block_reason', None)\n",
    "            \n",
    "            if block_reason and block_reason.name != \"BLOCK_REASON_UNSPECIFIED\":\n",
    "                reason = block_reason.name\n",
    "            \n",
    "            elif response.candidates and response.candidates[0].finish_reason.name:\n",
    "                reason = response.candidates[0].finish_reason.name\n",
    "\n",
    "            answer = f\"BLOCKED/{reason}\" \n",
    "\n",
    "        # Store the result\n",
    "        highstress_df.at[index, 'wg_followup'] = answer\n",
    "        print(f\"Index {index}: Result stored as '{answer}'\")\n",
    "\n",
    "    except APIError as e:\n",
    "        print(f\"API Error at index {index}: Status {e.status_code}\")\n",
    "        highstress_df.at[index, 'wg_followup'] = f'API_ERROR_{e.status_code}'\n",
    "    except Exception as e:\n",
    "        print(f\"General/Unexpected Error at index {index}: {e}\")\n",
    "        highstress_df.at[index, 'wg_followup'] = 'UNEXPECTED_ERROR'\n",
    "\n",
    "print(highstress_df.iloc[1:6][['agg_message_text', 'wg_followup']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
